var clone = require('clone.js');
var zlib = require('zlib')
var utils = require('utils')
var pf = require('packFile.js')
var fs = require('fs');
var co = require('gitCheckout');
var gi = require('gitInitCommand');
var rs = require('refs');

//copied from packFile:
var types = { 
	0 : 'invalid',
	1 : 'commit',
	2 : 'tree',
	3 : 'blob',
	4 : 'tag',
	5 : 'invalid',
	6 : 'delta1', // DELTA_ENCODED object w/ offset to base
	7 : 'delta2'  // DELTA_ENCODED object w/ base BINARY_OBJ_ID
}

/**
 * based on packFile:
 * Read object data header from begining of the buffer
 * @param buffer 
 * @return array [type of object, where the object starts, inflated length of data]
 */
var getLenType = function(buffer)
{
    var index = 0;
    var firstByte = buffer[index];
    index += 1;

    var length = firstByte & (1 + 2 + 4 + 8);//bits 0-3
    var typeCode = (firstByte & (16 + 32 + 64)) / 16;//bits 6-4
    var type = types[typeCode];
    var moreFlag = (firstByte & 128)/128;
    var coeff = 16;
    while (moreFlag !== 0)
    {
            var nextByte = buffer[index];
            index += 1;
            moreFlag = nextByte & 128;
            var lengthBits = nextByte & (~128);
            length += lengthBits * coeff;
            coeff *= 128;// *2^7
    }
    //console.log("type code: "+typeCode);
    return [type, index, length];
}


/**
 * check if buffer contains good pack 
 * TODO check sha1 at the end and rename it to checkPack 
 */
var checkSignature = function(data)
{
    var toComp = new Buffer("PACK");
    for(var i = 0; i < toComp.length; i++)
    {
        if(data[i] != toComp[i])
            return false
    }
    return true;
}
/**
 * helper for reading version and numbers of objects in pack
 */
var read4ByteNetwOrd = function(buf)
{
    var res = 0;
    for(var i = 0; i < 4; i++)
    {
        res *= 256; //16^2
        res += buf[i];
    }
    return res;
}
/**
 * check if whole pref is prefix of buffer
 */
var prefix = function(pref, buf)
{
    if(pref.length > buf.length)
        return false;
    for(var i = 0; i < pref.length; i++)
    {
        if(pref[i] != buf[i])
        {
            return false;
        }
    }
    return true;
}

/**
 * Class for holding data of object data in packfile 
 */
objectInPack = function(object_data, data_type, header_offset, data_offset, packedLen, unpackedLen, base_offset)
{
    this.object_data = object_data;
    this.data_type = data_type;
    this.header_offset = header_offset;
    this.data_offset = data_offset;
    this.packedLen = packedLen;
    this.unpackedLen = unpackedLen;
    this.base_offset = base_offset; //ofset from data.
}

/**
 * seeks through packfile and gather data about object_data, if something fails
 * calls wrongCompression to change to lvl+1 or produce error 
 * @param objData (buffer) packfile without hedar, intact through seek
 * @param cutData buffer with data to be parsed
 * @param elems how many elements are in packfile
 * @param parsed how many elements were parsed
 * @param prevs array[objectInPack] - information about parsed object data
 * @param callback([objectInPack], levelOfCompression)
 */
var seek = function(objData, cutData, elems, parsed, lvl, prevs, callback)
{
    if(parsed == elems)
    {
        if(cutData.length == 20)
        {
            callback(undefined, prevs);
            return;
        }
        else
        {
            wrongCompression(objData, elems, lvl, callback)();
        }
    }
    var len = getLenType(cutData, 0);
    console.log(len);
    var inflatedLength = len[2];
    var cut = len[1];
    var type = len[0];
    var ans = [];
    var inflate = zlib.createInflate();
    var dellen;
    if(type === 'delta1')
    {
        dellen = pf.getBaseObjectOffset(cutData.slice(cut, cutData.length), objData.length - cutData.length - cut);
        cut += dellen[0];
    }
    else if(type === 'delta2')
    {
        cut += 20; //20 byte base name
    }
    cutData = cutData.slice(cut, cutData.length);
    inflate.on('end', function(){
        inflate = null;
        var last = Buffer.concat(ans);
        if(last.length != inflatedLength)
        {
            wrongCompression(objData, elems, lvl, callback)();
        }
        utils.deflateOpts(last, function(err, deflatedData){
            if(err)
                throw err;
            else
            {
                if((!prefix(deflatedData, cutData)) || elems < parsed)
                {
                   wrongCompression(objData, elems, lvl, callback)(); //YUP, calling closure;
                }
                else
                {
                    if(type !== 'delta1')
                        prevs.push(new objectInPack(last, type, objData.length - cutData.length - cut, 
                            objData.length - cutData.length, deflatedData.length, inflatedLength));
                    else
                        prevs.push(new objectInPack(last, type, objData.length - cutData.length - cut, 
                            objData.length - cutData.length , deflatedData.length, inflatedLength, dellen[1] + 1));
                    seek2(objData, cutData.slice(deflatedData.length, cutData.length),
                    elems, parsed+1, -1, prevs, callback);
                }
            }
        }, {level : lvl} );
    });
    inflate.on('data', function(data){
        ans.push(data);
        
    });
    inflate.on('error', wrongCompression(objData, elems, lvl, callback));
    inflate.write(cutData);
    inflate.end();
}

/**
 * called when seek through pack fails, iterates to next level of compression
 * (closure)
 */
var wrongCompression = function(objData, elems, level, callback)
{
    var l = level;
    var o = objData;
    var e = elems;
    return function(err)
    {
        //TODO: shoud ignore this error?
//        if(err)
//        {
//            callback(err);
//        }
        if(l == 9)
            seek2(o, o, e, 0, -1, [], callback);
        else
        {
            seek(o, o, e, 0, l+1, [], callback);
        }
    }
}

/**
 * seeks through packfile and gather data about object_data, iterates for evey 
 * object with compression level from -1 to 9
 * @param objData (buffer) packfile without hedar, intact through seek
 * @param cutData buffer with data to be parsed
 * @param elems how many elements are in packfile
 * @param parsed how many elements were parsed
 * @param prevs array[objectInPack] - information about parsed object data
 * @param callback([objectInPack], levelOfCompression)
 */
var seek2 = function(objData, cutData, elems, parsed, lvl, prevs, callback)
{
    if(parsed == elems)
    {
        if(cutData.length == 20)
        {
            callback(undefined, prevs);
            return;
        }
        else
            wrongCompression2(objData, elems, lvl, callback)();
    }
    var len = getLenType(cutData, 0);
    var inflatedLength = len[2];
    var cut = len[1];
    var type = len[0];
    var ans = [];
    var inflate = zlib.createInflate();
    var dellen;
    if(type === 'delta1')
    {
        dellen = pf.getBaseObjectOffset(cutData.slice(cut, cutData.length), objData.length - cutData.length - cut);
        cut += dellen[0];
    }
    else if(type === 'delta2')
    {
        cut += 20; //20 byte base name
    }
    cutData = cutData.slice(cut, cutData.length);
    inflate.on('end', function(){
        inflate = null;
        var last = Buffer.concat(ans);
        if(last.length != inflatedLength) // do not check for deltas?
        {
            wrongCompression2(callback)('wrong inflation length: '+ last.length + " != " + inflatedLength);
         }
        deflateIter(last, -1, objData, cutData, elems, parsed, prevs, callback, type, inflatedLength, dellen, cut);
    });
    inflate.on('data', function(data){
        ans.push(data);
    });
    inflate.on('error', wrongCompression2(callback));
    inflate.write(cutData);
    inflate.end();
}

/**
 * Iterate level of deflation for one object
 */
var deflateIter = function(last, lvl, objData, cutData, elems, parsed, prevs, callback, type, inflatedLength, dellen, cut)
{
    utils.deflateOpts(last, function(err, deflatedData){
    if(err)
        throw err;
    else
    {
        if(!prefix(deflatedData, cutData))
        {
            if(lvl < 9)
                deflateIter(last, lvl + 1, objData, cutData, elems, parsed, prevs, callback, type, inflatedLength, dellen, cut);
        }
        else
        {
            if(type !== 'delta1')
                prevs.push(new objectInPack(last, type, objData.length - cutData.length - cut, 
                    objData.length - cutData.length, deflatedData.length, inflatedLength));
            else
                prevs.push(new objectInPack(last, type, objData.length - cutData.length - cut, 
                    objData.length - cutData.length , deflatedData.length, inflatedLength, dellen[1] + 1));
            seek2(objData, cutData.slice(deflatedData.length, cutData.length),
            elems, parsed+1, -1, prevs, callback);
        }
    }
    }, {level : lvl} );
}

var wrongCompression2 = function(callback)
{
    var c = callback;
    return function(err)
    {
        if(err)
            c(err);
    }
}


/**
 * start seekeing through packfile with -1 level of compression
 */
var callSeek = function(objData, elems, callback)
{
    seek(objData, objData, elems, 0, -1, [], callback);
}


/**
 * Parses one packfile
 * TODO, optional argument with level of compression
 * @param pack buufer with pack
 * @param callback(err, [objectInPack])
 */
var scanPack = function(pack, callback)
{
    //check for signature
    if(!checkSignature(pack))
    {
        callback('Not a valid packfile');
        return;
    }
    var version = read4ByteNetwOrd(pack.slice(4, 8));
    var elems = read4ByteNetwOrd(pack.slice(8, 12));
    var objData = pack.slice(12, pack.length);
    callSeek(objData, elems, function(err, data, lvl){
        callback(err, data);
    });
}

/**
 * Rebuilds objects from packfile
 * @param pack buffer with pack 
 * @param scan [objectInPack] - information about parsed object data
 */
var rebuildObjects = function(pack, scan)
{
    var fullObjects = new Object();
    var existingObjects = new Object();
    var deltas = [];
    scan.forEach(function(element){
        if(element.data_type === 'delta1' || element.data_type === 'delta2')
        {
            deltas['off'+element.header_offset] = element;
        }
        else
        {
            existingObjects['off'+element.header_offset] = element;
        }
    });
    while(Object.keys(deltas).length > 0)
    {
        for(var prop in deltas)
        {
            if(existingObjects['off'+deltas[prop].base_offset] !== undefined)
            {
                var base = existingObjects['off'+deltas[prop].base_offset];
                var newObj = pf.parseDeltaData(deltas[prop].object_data, base.object_data);
                existingObjects['off'+deltas[prop].header_offset] = 
                    new objectInPack(newObj, base.type, deltas[prop].header_offset, deltas[prop].data_offset, 0, newObj.length);
                delete deltas[prop];
            }
            
        }
    }
    for(var prop in existingObjects)
    {
        var elem = existingObjects[prop];
        var header = elem.data_type + " " + elem.unpackedLen +'\0';
        var head = new Buffer(header);
        var object = Buffer.concat([head, elem.object_data]);
        var sha1 = utils.getSha1(object);
        fullObjects['sha'+sha1] = object;
    }
    
    return fullObjects;
}

/**
 * Saves rebuild objects in repo
 * @param objects objects from rebuildObjects
 * @param repoPath string with path to repo
 */
var saveInRepo = function(objects, repoPath, callback)
{
    var len = Object.keys(objects).length;
    saveObject(objects, Object.keys(objects), repoPath, 0, len, callback);
}

var saveObject = function(objects, keys, repoPath, num, limit, callback)
{
    if(num < limit)
    {
        utils.saveObject(objects[keys[num]], repoPath, {},  function(err){
            if(err)
                throw err;
            saveObject(objects, keys, repoPath, num + 1, limit, callback);
        }); 
    }
    else
    {
        callback();
    }
}



/**
 * This does the whole clone
 * TODO one procedure for clone
 */
var test = function(packs, discovery)
{
    
    //var pck = fs.readFileSync("pack.pack");
    //console.log(prefix(pck, packs[0]));
    //console.log(prefix(packs[0], pck));
    console.log(packs);
    scanPack(packs[0], function(data, lvl){
        //console.log(data);
//        var header = "tree " + data.length +'\0';
//        var head = new Buffer(header);
//        var object = Buffer.concat([head, data]);
//        var sha1 = utils.getSha1(object);
//        console.log(sha1);
//        console.log(object);
//        //fs.openSync(path, flags, [mode])
//        //fs.writeFileSync("plik", object);
//        tree = fs.readFileSync("tree");
//        zlib.inflate(tree, function(err, buffer) {
//                for(i = 0; i < buffer.length; i++)
//                    console.log(i+ ' pack[i]: ' + object[i]+ ' tree[i]: '+ buffer[i]);
//            });
        var objects = rebuildObjects(packs[0], data);
        gi.init('test', function(err){
            var refs = new rs.Refs('test/.git', function(){
                    addBranches(refs, discovery, 0, discovery.headCount(), 'test/.git',
                        function(){
                            saveInRepo(objects, 'test/.git', function(){
                                if(discovery.hasValidHead())
                                    co.gitCheckout(discovery.headSha1(), 'test/.git', function(err){
                                    });
                                else if(discovery.headCount())
                                    co.gitCheckout(discovery.heads[0].sha1, 'test/.git', function(err){
                                    });
                            });
                        });
            });
        });
        
        console.log(objects);
        //saveInRepo(objects, "test");
    });
}

var ph = require('path');

var addBranches = function(refs, discovery, num, limit, repoPath, callback)
{
    if(num < limit)
    {
        refs.createBranch(discovery.heads[num].path.replace('refs/heads/','') , discovery.heads[num].sha1, function(err){
            if(err) 
                throw err;
            if((discovery.hasValidHead()) && discovery.heads[num].sha1 === discovery.headSha1())
            {
                fs.writeFile(ph.join(repoPath, 'HEAD'), new Buffer('ref: ' + discovery.heads[num].path), 
                    function(){
                        addBranches(refs, discovery, num+1, limit, repoPath, callback);
                    });
            }
            else
                addBranches(refs, discovery, num+1, limit, repoPath, callback);
        });
    }
    else
    {
        callback();
    }
}

//clone.getPacks('git://github.com/kyloel/clone.git', test);
clone.getPacks('git://github.com/kyloel/oriongit.git', test);
//clone.getPacks('git://github.com/torvalds/linux.git', test);
//clone.getPacks('git://github.com/kyloel/clone2.git', test);

exports.scanPack = scanPack;
exports.rebuildObjects = rebuildObjects;
exports.saveInRepo = saveInRepo;